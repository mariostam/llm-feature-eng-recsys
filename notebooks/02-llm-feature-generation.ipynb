{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wX06gHOO8uDH"
   },
   "source": [
    "# 2. LLM-Powered Feature Generation (Final, Robust Version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j3qZGQeO8uDR"
   },
   "source": [
    "### 1. Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import google.generativeai as genai\n",
    "import os\n",
    "import time\n",
    "from google.colab import drive, userdata\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pyarrow.parquet as pq\n",
    "from google.api_core import retry\n",
    "\n",
    "# --- Mount Google Drive ---\n",
    "print(\"Mounting Google Drive...\")\n",
    "drive.mount('/content/drive')\n",
    "print(\"Drive mounted.\")\n",
    "\n",
    "# --- Configure API using Colab Secrets ---\n",
    "print(\"Configuring Gemini API...\")\n",
    "try:\n",
    "    api_key = userdata.get('GEMINI_API_KEY')\n",
    "    genai.configure(api_key=api_key)\n",
    "    print(\"Gemini API configured successfully.\")\n",
    "except userdata.SecretNotFoundError:\n",
    "    raise ValueError(\"API key not found. Please add 'GEMINI_API_KEY' to Colab Secrets (ðŸ”‘ icon on the left).\")\n",
    "\n",
    "# --- Define File Paths ---\n",
    "input_path = '/content/drive/MyDrive/Colab Notebooks/llm-feature-engineering/data/master_dataframe.parquet'\n",
    "sample_output_path = '/content/drive/MyDrive/Colab Notebooks/llm-feature-engineering/data/sampled_df_with_llm_features.parquet'\n",
    "\n",
    "print(\"-\" * 50)\n",
    "print(f\"Input file: {input_path}\")\n",
    "print(f\"Sample output file: {sample_output_path}\")\n",
    "print(\"âœ… Setup complete. Ready to process.\")\n",
    "print(\"-\" * 50)"
   ],
   "metadata": {
    "id": "bubKurZS8uDR"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2. Prompt and Generation Function (with Exponential Backoff)"
   ],
   "metadata": {
    "id": "ERerGtDJt-XA"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# This is the DEFINITIVE version of the generation function.\n",
    "# 1. It disables the client library's default retry mechanism.\n",
    "# 2. It implements its own robust exponential backoff for retries.\n",
    "# 3. It includes a standard 5-second delay on success to respect the RPM limit.\n",
    "\n",
    "prompt_template = \"\"\"\n",
    "**Role**: You are a world-class film critic and movie analyst.\n",
    "**Instruction**: Analyze the provided movie title and plot overview to identify its core underlying themes.\n",
    "**Steps**: Distill these themes into a concise, comma-separated list of 5-7 thematic keywords. Focus on capturing the mood, central conflicts, and deeper meaning of the story. Avoid simply listing plot points.\n",
    "**End Goal**: Return ONLY the comma-separated list of keywords.\n",
    "\n",
    "---\n",
    "**Movie Title**: The Matrix\n",
    "**Plot Overview**: A computer hacker learns from mysterious rebels about the true nature of his reality and his role in the war against its controllers.\n",
    "**Thematic Keywords**: simulated reality, dystopian future, chosen one, rebellion, philosophical, cyberpunk\n",
    "---\n",
    "**Movie Title**: Forrest Gump\n",
    "**Plot Overview**: The presidencies of Kennedy and Johnson, the Vietnam War, the Watergate scandal and other historical events unfold from the perspective of an Alabama man with an IQ of 75, whose only desire is to be reunited with his childhood sweetheart.\n",
    "**Thematic Keywords**: historical epic, innocence, destiny, love, American history, serendipity\n",
    "---\n",
    "\n",
    "Now, generate the keywords for the following movie:\n",
    "**Movie Title**: {title}\n",
    "**Plot Overview**: {overview}\n",
    "**Thematic Keywords**:\n",
    "\"\"\"\n",
    "\n",
    "model = genai.GenerativeModel('gemini-1.5-flash')\n",
    "\n",
    "def generate_llm_keywords(title, overview, max_retries=5):\n",
    "    # Define a custom retry predicate. We only want to retry on 429 errors.\n",
    "    def is_retryable(e):\n",
    "        return isinstance(e, Exception) and \"429\" in str(e)\n",
    "\n",
    "    # Create a custom retry object with exponential backoff.\n",
    "    # It will wait 10s, then 20s, then 40s, etc.\n",
    "    custom_retry = retry.Retry(predicate=is_retryable, initial=10.0, maximum=300.0, multiplier=2.0)\n",
    "\n",
    "    try:\n",
    "        # Pass the custom retry object to the request.\n",
    "        response = model.generate_content(\n",
    "            prompt_template.format(title=title, overview=overview),\n",
    "            request_options={'retry': custom_retry}\n",
    "        )\n",
    "        # On success, wait 5 seconds to respect the 15 RPM limit.\n",
    "        time.sleep(5)\n",
    "        return response.text.strip()\n",
    "    except Exception as e:\n",
    "        if \"quota\" in str(e).lower():\n",
    "            print(f\"DAILY QUOTA EXCEEDED. Cannot process '{title}'. Please wait for the quota to reset.\")\n",
    "            # If we hit a quota error, we must stop and return an error.\n",
    "            return \"ERROR: Daily quota exceeded\"\n",
    "        else:\n",
    "            print(f\"An unrecoverable error occurred for '{title}': {e}\")\n",
    "            return \"ERROR: Unrecoverable API error\"\n",
    "\n",
    "print(\"âœ… Robust keyword generation function defined.\")"
   ],
   "metadata": {
    "id": "EQp_esJqt733"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TQMLnV-98uDT"
   },
   "source": [
    "### 3. Sampling and Sequential Processing"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# This cell creates a random sample and processes it sequentially.\n",
    "# It uses a standard for-loop to guarantee one-at-a-time processing.\n",
    "\n",
    "# --- Configuration ---\n",
    "SAMPLE_SIZE = 10_000\n",
    "\n",
    "# --- 1. Open a connection to the full Parquet file ---\n",
    "print(f\"Opening connection to full Parquet file: {input_path}\")\n",
    "parquet_file = pq.ParquetFile(input_path)\n",
    "total_rows = parquet_file.metadata.num_rows\n",
    "print(f\"Total rows available in file: {total_rows:,}\")\n",
    "\n",
    "# --- 2. Create a random sample of the data ---\n",
    "print(f\"\nCreating a random sample of {SAMPLE_SIZE:,} rows...\")\n",
    "\n",
    "# Memory-efficient sampling by iterating through chunks\n",
    "sampling_fraction = SAMPLE_SIZE / total_rows\n",
    "sample_chunks = []\n",
    "for chunk in parquet_file.iter_batches(batch_size=200_000):\n",
    "    chunk_df = chunk.to_pandas()\n",
    "    sampled_chunk = chunk_df.sample(frac=sampling_fraction, random_state=42)\n",
    "    sample_chunks.append(sampled_chunk)\n",
    "\n",
    "sample_df = pd.concat(sample_chunks, ignore_index=True)\n",
    "sample_df = sample_df.head(SAMPLE_SIZE)\n",
    "\n",
    "print(f\"Successfully created a sample of {len(sample_df):,} rows.\")\n",
    "\n",
    "# --- 3. Check for and resume a previously stopped job ---\n",
    "start_index = 0\n",
    "if os.path.exists(sample_output_path):\n",
    "    processed_df = pd.read_parquet(sample_output_path)\n",
    "    start_index = len(processed_df)\n",
    "    print(f\"Resuming from index {start_index} of the sample.\")\n",
    "    # To resume, we need to align the already processed data with the sample\n",
    "    sample_df = sample_df.iloc[start_index:].copy()\n",
    "    print(f\"Starting to process {len(sample_df):,} remaining rows.\")\n",
    "else:\n",
    "    print(\"Starting a new processing job.\")\n",
    "\n",
    "# --- 4. Process the SAMPLE DataFrame using a sequential for loop ---\n",
    "if not sample_df.empty:\n",
    "    # Create an empty list to store the results\n",
    "    llm_keywords_list = []\n",
    "\n",
    "    # Use tqdm directly on the iterator to show progress\n",
    "    for index, row in tqdm(sample_df.iterrows(), total=len(sample_df), desc=\"Processing Rows\"):\n",
    "        # Call the function for each row, one by one\n",
    "        keywords = generate_llm_keywords(row['title'], row['plot_overview'])\n",
    "        llm_keywords_list.append(keywords)\n",
    "        \n",
    "        # If we hit a hard quota error, stop the entire process.\n",
    "        if \"quota\" in keywords:\n",
    "            print(\"Stopping execution due to daily quota limit.\")\n",
    "            break\n",
    "\n",
    "    # Assign the results back to the DataFrame\n",
    "    processed_chunk = sample_df.iloc[:len(llm_keywords_list)].copy()\n",
    "    processed_chunk['llm_keywords'] = llm_keywords_list\n",
    "\n",
    "    # --- 5. Append the fully processed chunk to the output file ---\n",
    "    if start_index == 0:\n",
    "        # If we started from scratch, create the new file\n",
    "        processed_chunk.to_parquet(sample_output_path, engine='pyarrow', index=False)\n",
    "    else:\n",
    "        # If we resumed, append to the existing file\n",
    "        processed_chunk.to_parquet(sample_output_path, engine='pyarrow', index=False, append=True)\n",
    "\n",
    "print(\"-\" * 50)\n",
    "print(\"âœ… Processing finished.\")\n",
    "print(\"-\" * 50)"
   ],
   "metadata": {
    "id": "xdXBVI4u8uDT"
   },
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.12"
  },
  "colab": {
   "provenance": []
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}