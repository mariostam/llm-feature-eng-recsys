{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wX06gHOO8uDH"
   },
   "source": [
    "# LLM-Powered Feature Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j3qZGQeO8uDR"
   },
   "source": [
    "### Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bubKurZS8uDR"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import google.generativeai as genai\n",
    "import os\n",
    "import time\n",
    "from google.colab import drive, userdata\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pyarrow.parquet as pq\n",
    "from google.api_core import retry\n",
    "\n",
    "# Mount Google Drive to access project files.\n",
    "drive.mount('/content/drive')\n",
    "print(\"Drive mounted.\")\n",
    "\n",
    "# Securely configure the Gemini API using Colab Secrets.\n",
    "print(\"Configuring Gemini API...\")\n",
    "try:\n",
    "    api_key = userdata.get('GEMINI_API_KEY')\n",
    "    genai.configure(api_key=api_key)\n",
    "    print(\"Gemini API configured successfully.\")\n",
    "except userdata.SecretNotFoundError:\n",
    "    raise ValueError(\"API key not found. Please add 'GEMINI_API_KEY' to Colab Secrets (ðŸ”‘ icon on the left).\")\n",
    "\n",
    "# Define input and output file paths in Google Drive.\n",
    "input_path = '/content/drive/MyDrive/Colab Notebooks/llm-feature-engineering/data/master_dataframe.parquet'\n",
    "sample_output_path = '/content/drive/MyDrive/Colab Notebooks/llm-feature-engineering/data/sampled_df_with_llm_features.parquet'\n",
    "\n",
    "print(\"-\" * 50)\n",
    "print(f\"Input file: {input_path}\")\n",
    "print(f\"Sample output file: {sample_output_path}\")\n",
    "print(\"âœ… Setup complete. Ready to process.\")\n",
    "print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ERerGtDJt-XA"
   },
   "source": [
    "### Prompt and Generation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EQp_esJqt733"
   },
   "outputs": [],
   "source": [
    "# Definitive generation function with custom exponential backoff and rate limit delay.\n",
    "prompt_template = \"\"\"\n",
    "**Role**: You are a world-class film critic and movie analyst.\n",
    "**Instruction**: Analyze the provided movie title and plot overview to identify its core underlying themes.\n",
    "**Steps**: Distill these themes into a concise, comma-separated list of 5-7 thematic keywords. Focus on capturing the mood, central conflicts, and deeper meaning of the story. Avoid simply listing plot points.\n",
    "**End Goal**: Return ONLY the comma-separated list of keywords.\n",
    "\n",
    "---\n",
    "**Movie Title**: The Matrix\n",
    "**Plot Overview**: A computer hacker learns from mysterious rebels about the true nature of his reality and his role in the war against its controllers.\n",
    "**Thematic Keywords**: simulated reality, dystopian future, chosen one, rebellion, philosophical, cyberpunk\n",
    "---\n",
    "**Movie Title**: Forrest Gump\n",
    "**Plot Overview**: The presidencies of Kennedy and Johnson, the Vietnam War, the Watergate scandal and other historical events unfold from the perspective of an Alabama man with an IQ of 75, whose only desire is to be reunited with his childhood sweetheart.\n",
    "**Thematic Keywords**: historical epic, innocence, destiny, love, American history, serendipity\n",
    "---\n",
    "\n",
    "Now, generate the keywords for the following movie:\n",
    "**Movie Title**: {title}\n",
    "**Plot Overview**: {overview}\n",
    "**Thematic Keywords**:\n",
    "\"\"\"\n",
    "\n",
    "model = genai.GenerativeModel('gemini-1.5-flash')\n",
    "\n",
    "def generate_llm_keywords(title, overview, max_retries=5):\n",
    "    # Define a custom retry predicate for rate limit errors.\n",
    "    def is_retryable(e):\n",
    "        return isinstance(e, Exception) and \"429\" in str(e)\n",
    "\n",
    "    # Configure exponential backoff for retries.\n",
    "    custom_retry = retry.Retry(predicate=is_retryable, initial=10.0, maximum=300.0, multiplier=2.0)\n",
    "\n",
    "    try:\n",
    "        # Generate content with the custom retry configuration.\n",
    "        response = model.generate_content(\n",
    "            prompt_template.format(title=title, overview=overview),\n",
    "            request_options={'retry': custom_retry}\n",
    "        )\n",
    "        # Pause to respect the API's rate limit.\n",
    "        time.sleep(5)\n",
    "        return response.text.strip()\n",
    "    except Exception as e:\n",
    "        if \"quota\" in str(e).lower():\n",
    "            print(f\"DAILY QUOTA EXCEEDED. Cannot process '{title}'. Please wait for the quota to reset.\")\n",
    "            # Stop processing if the daily quota is exceeded.\n",
    "            return \"ERROR: Daily quota exceeded\"\n",
    "        else:\n",
    "            print(f\"An unrecoverable error occurred for '{title}': {e}\")\n",
    "            return \"ERROR: Unrecoverable API error\"\n",
    "\n",
    "print(\"âœ… Robust keyword generation function defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TQMLnV-98uDT"
   },
   "source": [
    "### Sampling and Sequential Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xdXBVI4u8uDT"
   },
   "outputs": [],
   "source": [
    "# Creates a random sample and processes it sequentially, one row at a time.\n",
    "# A standard for-loop guarantees one-at-a-time processing.\n",
    "\n",
    "# --- Configuration ---\n",
    "SAMPLE_SIZE = 10_000\n",
    "\n",
    "# Open a connection to the full Parquet file.\n",
    "print(f\"Opening connection to full Parquet file: {input_path}\")\n",
    "parquet_file = pq.ParquetFile(input_path)\n",
    "total_rows = parquet_file.metadata.num_rows\n",
    "print(f\"Total rows available in file: {total_rows:,}\")\n",
    "\n",
    "# Create a random sample of the data.\n",
    "print(f\"\\nCreating a random sample of {SAMPLE_SIZE:,} rows...\")\n",
    "\n",
    "# Sample data efficiently by iterating through chunks.\n",
    "sampling_fraction = SAMPLE_SIZE / total_rows\n",
    "sample_chunks = []\n",
    "for chunk in parquet_file.iter_batches(batch_size=200_000):\n",
    "    chunk_df = chunk.to_pandas()\n",
    "    sampled_chunk = chunk_df.sample(frac=sampling_fraction, random_state=42)\n",
    "    sample_chunks.append(sampled_chunk)\n",
    "\n",
    "sample_df = pd.concat(sample_chunks, ignore_index=True)\n",
    "sample_df = sample_df.head(SAMPLE_SIZE)\n",
    "\n",
    "print(f\"Successfully created a sample of {len(sample_df):,} rows.\")\n",
    "\n",
    "# Check for and resume a previously stopped job.\n",
    "start_index = 0\n",
    "if os.path.exists(sample_output_path):\n",
    "    processed_df = pd.read_parquet(sample_output_path)\n",
    "    start_index = len(processed_df)\n",
    "    print(f\"Resuming from index {start_index} of the sample.\")\n",
    "    # Align the sample with the already processed data to resume.\n",
    "    sample_df = sample_df.iloc[start_index:].copy()\n",
    "    print(f\"Starting to process {len(sample_df):,} remaining rows.\")\n",
    "else:\n",
    "    print(\"Starting a new processing job.\")\n",
    "\n",
    "# Process the sample sequentially using a for loop.\n",
    "if not sample_df.empty:\n",
    "    # Store the generated keywords in a list.\n",
    "    llm_keywords_list = []\n",
    "\n",
    "    # Use tqdm for a progress bar.\n",
    "    for index, row in tqdm(sample_df.iterrows(), total=len(sample_df), desc=\"Processing Rows\"):\n",
    "        # Generate keywords for each row.\n",
    "        keywords = generate_llm_keywords(row['title'], row['plot_overview'])\n",
    "        llm_keywords_list.append(keywords)\n",
    "        \n",
    "        # Stop processing if the daily quota is exceeded.\n",
    "        if \"quota\" in keywords:\n",
    "            print(\"Stopping execution due to daily quota limit.\")\n",
    "            break\n",
    "\n",
    "    # Assign the results back to the DataFrame\n",
    "    processed_chunk = sample_df.iloc[:len(llm_keywords_list)].copy()\n",
    "    processed_chunk['llm_keywords'] = llm_keywords_list\n",
    "\n",
    "    # Append the processed chunk to the output file.\n",
    "    if start_index == 0:\n",
    "        # Create a new file if starting from scratch.\n",
    "        processed_chunk.to_parquet(sample_output_path, engine='pyarrow', index=False)\n",
    "    else:\n",
    "        # Append to the existing file if resuming.\n",
    "        processed_chunk.to_parquet(sample_output_path, engine='pyarrow', index=False, append=True)\n",
    "\n",
    "print(\"-\" * 50)\n",
    "print(\"âœ… Processing finished.\")\n",
    "print(\"-\" * 50)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
