{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "69cc9fe0",
   "metadata": {},
   "source": [
    "\n",
    "# 03 - Factorization Machine Model Experiment\n",
    "\n",
    "This notebook contains the complete code to run the Factorization Machine experiment, comparing the model trained with human-generated keywords against the model trained with LLM-generated keywords.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c43b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from scipy.sparse import hstack\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def main():\n",
    "    print('--- 1. Loading and Preprocessing Data ---')\n",
    "    DATA_PATH = 'data/final_dataset_with_llm_keywords.parquet'\n",
    "\n",
    "    try:\n",
    "        df = pd.read_parquet(DATA_PATH)\n",
    "        print(f\"Real dataset loaded successfully. Shape: {df.shape}\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Warning: The data file was not found at {DATA_PATH}\")\n",
    "        print('Using a dummy dataframe for demonstration purposes.')\n",
    "        df = pd.DataFrame({\n",
    "            'user_id': [1, 1, 2, 2, 3, 3, 4, 4, 1, 2, 3, 4, 1, 2, 3, 4],\n",
    "            'movie_id': [101, 102, 101, 103, 102, 104, 103, 104, 105, 106, 105, 106, 107, 108, 107, 108],\n",
    "            'rating': [5, 3, 4, 2, 5, 4, 3, 5, 2, 4, 3, 5, 4, 3, 2, 5],\n",
    "            'human_keywords': ['action, thriller', 'comedy, romance', 'action, thriller', 'sci-fi, adventure', 'comedy, romance', 'drama', 'sci-fi, adventure', 'drama', 'mystery', 'fantasy', 'mystery', 'fantasy', 'crime', 'history', 'crime', 'history'],\n",
    "            'llm_keywords': ['fast-paced, explosive', 'lighthearted, love', 'fast-paced, explosive', 'space, future', 'lighthearted, love', 'emotional, serious', 'space, future', 'emotional, serious', 'suspenseful, investigation', 'magical, mythical', 'suspenseful, investigation', 'magical, mythical', 'gritty, investigation', 'period piece, factual', 'gritty, investigation', 'period piece, factual']\n",
    "        })\n",
    "\n",
    "    print('\\n--- 2. Performing Feature Engineering ---')\n",
    "    df['user_id_cat'] = df['user_id'].astype('category').cat.codes\n",
    "    df['movie_id_cat'] = df['movie_id'].astype('category').cat.codes\n",
    "\n",
    "    def create_feature_matrix(df, keywords_column):\n",
    "        df[keywords_column] = df[keywords_column].fillna('').str.replace(',', ' ')\n",
    "        vectorizer = CountVectorizer()\n",
    "        keyword_bow = vectorizer.fit_transform(df[keywords_column])\n",
    "        user_ohe = pd.get_dummies(df['user_id_cat'], prefix='user', sparse=True)\n",
    "        movie_ohe = pd.get_dummies(df['movie_id_cat'], prefix='movie', sparse=True)\n",
    "        features_sparse = hstack([user_ohe, movie_ohe, keyword_bow], format='csr')\n",
    "        return features_sparse\n",
    "\n",
    "    X_human = create_feature_matrix(df.copy(), 'human_keywords')\n",
    "    X_llm = create_feature_matrix(df.copy(), 'llm_keywords')\n",
    "    y = df['rating'].values\n",
    "\n",
    "    print(f\"Control (Human) Feature Matrix Shape: {X_human.shape}\")\n",
    "    print(f\"Experimental (LLM) Feature Matrix Shape: {X_llm.shape}\")\n",
    "\n",
    "    def create_dataloaders(X, y, test_size=0.2, batch_size=1024, random_state=42):\n",
    "        indices = np.arange(X.shape[0])\n",
    "        train_indices, test_indices = train_test_split(indices, test_size=test_size, random_state=random_state)\n",
    "        X_train, X_test = X[train_indices], X[test_indices]\n",
    "        y_train, y_test = y[train_indices], y[test_indices]\n",
    "        X_train_tensor = torch.from_numpy(X_train.toarray()).float()\n",
    "        X_test_tensor = torch.from_numpy(X_test.toarray()).float()\n",
    "        y_train_tensor = torch.from_numpy(y_train).float().view(-1, 1)\n",
    "        y_test_tensor = torch.from_numpy(y_test).float().view(-1, 1)\n",
    "        train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "        test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "        return train_loader, test_loader\n",
    "\n",
    "    train_loader_human, test_loader_human = create_dataloaders(X_human, y)\n",
    "    train_loader_llm, test_loader_llm = create_dataloaders(X_llm, y)\n",
    "    print('\\nDataLoaders created successfully.')\n",
    "\n",
    "    class FactorizationMachine(nn.Module):\n",
    "        def __init__(self, num_features, embedding_dim=10):\n",
    "            super(FactorizationMachine, self).__init__()\n",
    "            self.bias = nn.Parameter(torch.randn(1))\n",
    "            self.weights = nn.Linear(num_features, 1, bias=False)\n",
    "            self.embeddings = nn.Linear(num_features, embedding_dim, bias=False)\n",
    "        def forward(self, x):\n",
    "            linear_part = self.bias + self.weights(x)\n",
    "            embedded_x = self.embeddings(x)\n",
    "            sum_of_squares = embedded_x.pow(2).sum(1, keepdim=True)\n",
    "            square_of_sum = self.embeddings(x.pow(2)).sum(1, keepdim=True)\n",
    "            factorization_part = 0.5 * (sum_of_squares - square_of_sum)\n",
    "            return linear_part + factorization_part\n",
    "\n",
    "    def train_model(model, train_loader, optimizer, criterion, device):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for features, targets in train_loader:\n",
    "            features, targets = features.to(device), targets.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            predictions = model(features)\n",
    "            loss = criterion(predictions, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item() * len(targets)\n",
    "        return total_loss / len(train_loader.dataset)\n",
    "\n",
    "    def evaluate_model(model, test_loader, criterion, device):\n",
    "        model.eval()\n",
    "        total_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for features, targets in test_loader:\n",
    "                features, targets = features.to(device), targets.to(device)\n",
    "                predictions = model(features)\n",
    "                loss = criterion(predictions, targets)\n",
    "                total_loss += loss.item() * len(targets)\n",
    "        mse = total_loss / len(test_loader.dataset)\n",
    "        return np.sqrt(mse)\n",
    "\n",
    "    def run_experiment(train_loader, test_loader, num_features, num_epochs=10, learning_rate=0.01, embedding_dim=50):\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        print(f\"Using device: {device}\")\n",
    "        model = FactorizationMachine(num_features=num_features, embedding_dim=embedding_dim).to(device)\n",
    "        criterion = nn.MSELoss()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "        for epoch in tqdm(range(num_epochs), desc=\"Training Epochs\"):\n",
    "            train_loss = train_model(model, train_loader, optimizer, criterion, device)\n",
    "            test_rmse = evaluate_model(model, test_loader, criterion, device)\n",
    "            print(f\"Epoch {epoch+1}/{num_epochs} | Train Loss: {train_loss:.4f} | Test RMSE: {test_rmse:.4f}\")\n",
    "        return evaluate_model(model, test_loader, criterion, device)\n",
    "\n",
    "    print('\\n--- 3. Starting Control Group Experiment (Human Keywords) ---')\n",
    "    num_features_human = X_human.shape[1]\n",
    "    rmse_human = run_experiment(train_loader_human, test_loader_human, num_features_human)\n",
    "    print(f\"\\\\nFinal RMSE for Control (Human) Model: {rmse_human:.4f}\")\n",
    "\n",
    "    print('\\n--- 4. Starting Experimental Group Experiment (LLM Keywords) ---')\n",
    "    num_features_llm = X_llm.shape[1]\n",
    "    rmse_llm = run_experiment(train_loader_llm, test_loader_llm, num_features_llm)\n",
    "    print(f\"\\\\nFinal RMSE for Experimental (LLM) Model: {rmse_llm:.4f}\")\n",
    "\n",
    "    print('\\n========== 5. EXPERIMENT RESULTS ==========')\n",
    "    print(f\"RMSE (Human Keywords): {rmse_human:.4f}\")\n",
    "    print(f\"RMSE (LLM Keywords):   {rmse_llm:.4f}\")\n",
    "    print('========================================')\n",
    "\n",
    "    improvement = rmse_human - rmse_llm\n",
    "    if improvement > 0.0001:\n",
    "        print(f\"\\\\nHypothesis Confirmed: LLM-based model performed better by an RMSE of {improvement:.4f}.\")\n",
    "    elif improvement < -0.0001:\n",
    "        print(f\"\\\\nHypothesis Rejected: Human-based model performed better by an RMSE of {-improvement:.4f}.\")\n",
    "    else:\n",
    "        print('\\nResult: No significant difference in model performance.')\n",
    "\n",
    "main()\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
